{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "631eccf85cdbbcfa",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9636b4ee208afb9f",
   "metadata": {},
   "source": [
    "# <span style=\"color:orange; font-weight:bold;\">Regulatory AI Search Engine with Llama</span> <br>\n",
    "### <span style=\"color:orange; font-weight:bold;\">A Local Retrieval-Augmented Generation (RAG) </span> <br> <br>\n",
    "#### <span style=\"color:orange; font-weight:bold;\">Script to create and maintain the vectorDB </span> <br> <br>\n",
    "*created with ❤️ by Daniel Jacobowitz & Prasoon Singh*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d0a4d9e89b85eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24fae24c-433b-4860-badd-e0eb20759f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T09:34:22.361596Z",
     "start_time": "2024-10-29T09:34:22.341847Z"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install langchain-community==0.2.4 langchain==0.2.3 faiss-cpu==1.8.0 unstructured==0.14.5 unstructured[pdf]==0.14.5 transformers==4.41.2 sentence-transformers==3.0.1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import os # --> interacts with the operating system, like creating files and directories, management of files and directories, input, output, environment variables etc.\n",
    "\n",
    "#import pandas as pd # --> Analyses data and allows to manipulate data as dataframes\n",
    "\n",
    "from langchain_community.llms import Ollama  # --> LLM classes provide access to the large language model (in this case locally to Ollama) APIs and services.\n",
    "\n",
    "from langchain.document_loaders import UnstructuredFileLoader # --> Loads data from a source as Documents, which are a piece of text and associated metadata.\n",
    "                                                              # --> In this case, the file loader uses the unstructured partition function and will automatically detect the file type.\n",
    "\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader # --> Loads data from a source as Documents, which are a piece of text and associated metadata.\n",
    "                                                                # --> In this case, the file loader loads a directory with PDF files using pypdf and chunks at character level. The loader also stores page numbers in metadata.\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings #--> Convert the text of the PDF files using Embeddings models into numerical vectors\n",
    "#from langchain_community.embeddings.ollama import OllamaEmbeddings #--> Convert the text of the PDF files using Embeddings models into numerical vectors\n",
    "#from langchain_ollama import OllamaEmbeddings #--> Convert the text of the PDF files using Embeddings models into numerical vectors\n",
    "\n",
    "from langchain_community.vectorstores import FAISS # --> Creates a local vector store to store the created embeddings of the PDF files. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "\n",
    "from langchain.text_splitter import CharacterTextSplitter # --> Splits a long document (in our case the PDF files) into smaller chunks that can fit into the model`s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter # --> Splits a long document (in our case the PDF files) into smaller chunks that can fit into the model`s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "from langchain_experimental.text_splitter import SemanticChunker #--> Splits a long document (in our case the PDF files) into smaller chunks that can fit into the model`s context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.\n",
    "\n",
    "from langchain.chains import RetrievalQA # --> Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. This chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a response.\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate # --> A prompt for a language model is a set of instructions or input provided by a user to guide the model's response, helping it understand the context and generate relevant and coherent language-based output, such as answering questions, completing sentences, or engaging in a conversation.\n",
    "#from langchain.memory import ConversationBufferMemory # --> Creates a conversational memory that allows a LLM to remember previous interactions/ chats with the user. It helps to manage and store conversation history in a structured way and to maintain the context of a conversation over multiple interactions.\n",
    "\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler # --> LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. In this case, it is a callback for streaming.\n",
    "                                                                                # --> Streaming: process of incrementally receiving and processing data generated by a Large Language Model (LLM) as it is produced, rather than waiting for the entire response to be generated before displaying it.\n",
    "from langchain.callbacks.manager import CallbackManager # --> LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. In this case, it is to manage callbacks.\n",
    "\n",
    "import streamlit as st # --> Streamlit is a free and open-source framework to rapidly build and share beautiful machine learning and data science web apps.\n",
    "\n",
    "import time #This module provides various time-related functions. \n",
    "\n",
    "if not os.path.exists('C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/pdfFiles'): # Check if the folder does not exist\n",
    "    os.makedirs('C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/pdfFiles') # create a folder in the specified path if it does not exist already\n",
    "\n",
    "if not os.path.exists('C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/vectorDB'): # Check if the folder does not exist\n",
    "    os.makedirs('C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/vectorDB') # create a folder in the specified path if it does not exist already\n",
    "\n",
    "# to initialize a new vector database, change to 1\n",
    "switch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2617cb33f823ebfe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "378c5f69-192c-4db7-bc73-68d1fdbc1b2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T09:34:27.017089Z",
     "start_time": "2024-10-29T09:34:27.002832Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Upload relevant PDF files\n",
    "DATA_PATH = \"C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/pdfFiles\" # Directory where the relevant PDFs are stored\n",
    "\n",
    "if switch == 1:\n",
    "    def load_documents(DATA_PATH): # defines a function named 'load_documents' that takes one parameter, 'DATA_PATH'.\n",
    "        document_loader = PyPDFDirectoryLoader(DATA_PATH) # creates the class 'PyPDFDirectoryLoader' designed to load PDF documents from a specified directory.\n",
    "        return document_loader.load() # the load method on the document_loader instance and returns the result. The load method is expected to read and load the PDF documents from the directory specified by DATA_PATH. The load method loads data into Document objects.\n",
    "\n",
    "    documents = load_documents(DATA_PATH) # the variable 'documents' is a object containing for every list elements a page of the PDF files.\n",
    "    \n",
    "    # --> Plausibility check\n",
    "    print(\"\\033[92mThe upload of the PDF files was successful!\\033[0m\")\n",
    "    print('')\n",
    "    print('CRR:')\n",
    "    print (documents[0])\n",
    "    print('')\n",
    "    print('Banking Act:')\n",
    "    print(documents[337])\n",
    "    print('')\n",
    "    print('MaRisk:')\n",
    "    print(documents[600])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5d82c99-3c23-4870-b5ac-959cae3052e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T09:34:31.563955Z",
     "start_time": "2024-10-29T09:34:31.546910Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Chunking the documents\n",
    "\n",
    "if switch == 1:\n",
    "    #text_splitter = SemanticChunker(embeddings=HuggingFaceEmbeddings(), breakpoint_threshold_type=\"percentile\") # Apply the chunking based on cosine similarity. There a many breakpoint threshold types.\n",
    "    #text_chunks = text_splitter.split_documents(documents) # #  splits the documents into chunks based on the similarity\n",
    "    \n",
    "    text_splitter = RecursiveCharacterTextSplitter( # function to split documents into chunks\n",
    "                chunk_size=1500, # sets the maximum size of each chunk to 1500 characters.\n",
    "                chunk_overlap=200, #  sets the overlap between consecutive chunks to 200 characters. Overlapping helps to ensure that context is preserved across chunks.\n",
    "                length_function=len # specifies that the length of the chunks will be measured using the len function, which counts the number of characters.\n",
    "            )\n",
    "\n",
    "    text_chunks = text_splitter.split_documents(documents) #  splits the documents into chunks based on the specified chunk_size and chunk_overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7cb5e2be-76e5-49f3-a50a-d336be3d30e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T09:34:46.955854Z",
     "start_time": "2024-10-29T09:34:33.831988Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49171\\AppData\\Local\\Temp\\ipykernel_25452\\1986862643.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embeddings = HuggingFaceEmbeddings() # The used embeddings model is from HuggingFace\n",
      "C:\\Users\\49171\\AppData\\Local\\Temp\\ipykernel_25452\\1986862643.py:3: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings() # The used embeddings model is from HuggingFace\n",
      "C:\\Users\\49171\\PycharmProjects\\pythonProject3\\.venv\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "C:\\Users\\49171\\PycharmProjects\\pythonProject3\\.venv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 3. Convert the splitted text into Vector Embeddings incl. indexation and store the created vector data base locally\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings() # The used embeddings model is from HuggingFace\n",
    " #embeddings = OllamaEmbeddings(\n",
    "    #        model = \"llama3.2:3b\",\n",
    "    #  base_url = 'http://localhost:11434'\n",
    "    #    )\n",
    "\n",
    "if switch == 1:\n",
    "   # Create and save the vector store\n",
    "    knowledge_base = FAISS.from_documents(text_chunks, embeddings) # The created chunks are converted to numerical vectors and stored in a FAISS vector database.\n",
    "    knowledge_base.save_local('C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/vectorDB') # The created FAISS vector database is stored locally in the given directory.\n",
    "\n",
    "    print(\"\\033[92mThe Embedding was successful!\\033[0m\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a9c707b20093f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54ffbe732f2c1a23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T09:35:21.935885Z",
     "start_time": "2024-10-29T09:35:20.398886Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\49171\\AppData\\Local\\Temp\\ipykernel_25452\\2681690023.py:15: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  embeddings = HuggingFaceEmbeddings() # The used embeddings model is from HuggingFace\n"
     ]
    }
   ],
   "source": [
    "# this cell is to manage an existing the VectorDB\n",
    "\n",
    "db = FAISS.load_local( # This method converts the stored vector database into a retriever object. A retriever is typically used to search and retrieve relevant documents or information. In other words, it will be the base for the search as the retrieved information will then later be passed to the LLM.\n",
    "    'C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/vectorDB', # Directory of the locally stored vector database\n",
    "    embeddings, # Parameter to declare, that the embeddings should be loaded.\n",
    "    allow_dangerous_deserialization=True)\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter( # function to split documents into chunks\n",
    "                chunk_size=1500, # sets the maximum size of each chunk to 1500 characters.\n",
    "                chunk_overlap=200, #  sets the overlap between consecutive chunks to 200 characters. Overlapping helps to ensure that context is preserved across chunks.\n",
    "                length_function=len # specifies that the length of the chunks will be measured using the len function, which counts the number of characters.\n",
    "            )\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings() # The used embeddings model is from HuggingFace\n",
    " #embeddings = OllamaEmbeddings(\n",
    "    #        model = \"llama3.2:3b\",\n",
    "    #  base_url = 'http://localhost:11434'\n",
    "    #    )\n",
    "\n",
    "# function to display documents in vectorDB\n",
    "def show_vstore(store):\n",
    "    vector_df = store_to_df(store)\n",
    "    print(vector_df)\n",
    "\n",
    "# function to convert a vector store into df to convenient access\n",
    "def store_to_df(store):\n",
    "    v_dict = store.docstore._dict\n",
    "    data_rows = []\n",
    "    for k in v_dict.keys():\n",
    "        source_path = v_dict[k].metadata[\"source\"].split(\"/\")[-1]\n",
    "        source_filename = os.path.basename(source_path)\n",
    "        doc_name = source_filename\n",
    "        \n",
    "        page_number = v_dict[k].metadata[\"page\"]+1\n",
    "        \n",
    "        content = v_dict[k].page_content\n",
    "        \n",
    "        data_rows.append({\"chunk_id\":k, \"document\":doc_name,\"page\":page_number,\"content\":content})\n",
    "    vector_df = pd.DataFrame(data_rows)\n",
    "    return vector_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a1706a53a76e99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T09:35:27.124946Z",
     "start_time": "2024-10-29T09:35:27.055215Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>document</th>\n",
       "      <th>page</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d830feca-520c-496e-ab58-dcd58b70504f</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>I \\n(Legislative acts)  \\nREGULATIONS  \\nREGUL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af3b5a0c-494f-48ab-9a5c-843854c08145</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>stronger liquidity buffers. In response to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a688ad37-c9d3-43d0-b3cd-63ba9262a6ad</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>national regulatory measures considered to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3c44f466-4629-4acf-8952-93aad77b5eb2</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>the Council of 14 June 2006 on the capital ade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4bc26976-f6df-49f3-8384-7a4c98f286b7</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>concerning the access to the activity of insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257</th>\n",
       "      <td>cbc10643-ad89-4170-9b93-c258d8fc5ba1</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>117</td>\n",
       "      <td>the proposals for action with the responsible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>3cc3400d-138c-4013-8e91-4a3103b2b116</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>118</td>\n",
       "      <td>Bundesanstalt für Finanzdienstleistungsaufsich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>aebe2119-3ea9-4e44-9624-2e8edb1546e3</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>118</td>\n",
       "      <td>office, and\\ni)in the case of institutions wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>5ca7e410-6d7f-472c-bc3c-85a3849069f4</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>119</td>\n",
       "      <td>Bundesanstalt für Finanzdienstleistungsaufsich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>4a1d6443-e8de-42c4-9af8-eabeba23f29c</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>119</td>\n",
       "      <td>market-oriented institutions shall draw up the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4262 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  chunk_id    document  page  \\\n",
       "0     d830feca-520c-496e-ab58-dcd58b70504f     CRR.pdf     1   \n",
       "1     af3b5a0c-494f-48ab-9a5c-843854c08145     CRR.pdf     1   \n",
       "2     a688ad37-c9d3-43d0-b3cd-63ba9262a6ad     CRR.pdf     1   \n",
       "3     3c44f466-4629-4acf-8952-93aad77b5eb2     CRR.pdf     2   \n",
       "4     4bc26976-f6df-49f3-8384-7a4c98f286b7     CRR.pdf     2   \n",
       "...                                    ...         ...   ...   \n",
       "4257  cbc10643-ad89-4170-9b93-c258d8fc5ba1  MaRisk.pdf   117   \n",
       "4258  3cc3400d-138c-4013-8e91-4a3103b2b116  MaRisk.pdf   118   \n",
       "4259  aebe2119-3ea9-4e44-9624-2e8edb1546e3  MaRisk.pdf   118   \n",
       "4260  5ca7e410-6d7f-472c-bc3c-85a3849069f4  MaRisk.pdf   119   \n",
       "4261  4a1d6443-e8de-42c4-9af8-eabeba23f29c  MaRisk.pdf   119   \n",
       "\n",
       "                                                content  \n",
       "0     I \\n(Legislative acts)  \\nREGULATIONS  \\nREGUL...  \n",
       "1     stronger liquidity buffers. In response to the...  \n",
       "2     national regulatory measures considered to be ...  \n",
       "3     the Council of 14 June 2006 on the capital ade...  \n",
       "4     concerning the access to the activity of insti...  \n",
       "...                                                 ...  \n",
       "4257  the proposals for action with the responsible ...  \n",
       "4258  Bundesanstalt für Finanzdienstleistungsaufsich...  \n",
       "4259  office, and\\ni)in the case of institutions wit...  \n",
       "4260  Bundesanstalt für Finanzdienstleistungsaufsich...  \n",
       "4261  market-oriented institutions shall draw up the...  \n",
       "\n",
       "[4262 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test to see the vectorDB.\n",
    "show_vstore(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eb3a83400f930fcd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T09:41:19.863082Z",
     "start_time": "2024-10-29T09:39:50.673886Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mMissing documents has been identified.\u001b[0m\n",
      "\u001b[The following documents are missing:\u001b[0m ['MaRisk_v2.pdf']\n",
      "\u001b[91mThe missing documents will be added now to the vector database.\u001b[0m\n",
      "\n",
      "Please wait...\n",
      "\n",
      "\u001b[92mThe missing files has been added successfully to the vector database.\u001b[0m\n",
      "\n",
      "Now the missing files will be chunked.\n",
      "\n",
      "\u001b[92mThe text chunking was successful! Now the new chunks will be added to the existing vector database.\u001b[0m\n",
      "\n",
      "\u001b[92mThe merging was successful! The local vector database is now updated. See you next time!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>document</th>\n",
       "      <th>page</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d830feca-520c-496e-ab58-dcd58b70504f</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>I \\n(Legislative acts)  \\nREGULATIONS  \\nREGUL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af3b5a0c-494f-48ab-9a5c-843854c08145</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>stronger liquidity buffers. In response to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a688ad37-c9d3-43d0-b3cd-63ba9262a6ad</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>national regulatory measures considered to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3c44f466-4629-4acf-8952-93aad77b5eb2</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>the Council of 14 June 2006 on the capital ade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4bc26976-f6df-49f3-8384-7a4c98f286b7</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>concerning the access to the activity of insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4535</th>\n",
       "      <td>84e73b2c-5c3e-4388-9dbf-ee4dfe68449c</td>\n",
       "      <td>MaRisk_v2.pdf</td>\n",
       "      <td>117</td>\n",
       "      <td>the proposals for action with the responsible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4536</th>\n",
       "      <td>98ce08a3-50c7-47e9-bd96-6b28351baf28</td>\n",
       "      <td>MaRisk_v2.pdf</td>\n",
       "      <td>118</td>\n",
       "      <td>Bundesanstalt für Finanzdienstleistungsaufsich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4537</th>\n",
       "      <td>19701d48-d0bd-4a60-8513-07d9449a2b40</td>\n",
       "      <td>MaRisk_v2.pdf</td>\n",
       "      <td>118</td>\n",
       "      <td>office, and\\ni)in the case of institutions wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4538</th>\n",
       "      <td>6e2b0b8d-599a-4d3f-9077-0c453d99b02a</td>\n",
       "      <td>MaRisk_v2.pdf</td>\n",
       "      <td>119</td>\n",
       "      <td>Bundesanstalt für Finanzdienstleistungsaufsich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4539</th>\n",
       "      <td>6294185b-4686-4689-8bd6-12b7bbeedd4a</td>\n",
       "      <td>MaRisk_v2.pdf</td>\n",
       "      <td>119</td>\n",
       "      <td>market-oriented institutions shall draw up the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4540 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  chunk_id       document  page  \\\n",
       "0     d830feca-520c-496e-ab58-dcd58b70504f        CRR.pdf     1   \n",
       "1     af3b5a0c-494f-48ab-9a5c-843854c08145        CRR.pdf     1   \n",
       "2     a688ad37-c9d3-43d0-b3cd-63ba9262a6ad        CRR.pdf     1   \n",
       "3     3c44f466-4629-4acf-8952-93aad77b5eb2        CRR.pdf     2   \n",
       "4     4bc26976-f6df-49f3-8384-7a4c98f286b7        CRR.pdf     2   \n",
       "...                                    ...            ...   ...   \n",
       "4535  84e73b2c-5c3e-4388-9dbf-ee4dfe68449c  MaRisk_v2.pdf   117   \n",
       "4536  98ce08a3-50c7-47e9-bd96-6b28351baf28  MaRisk_v2.pdf   118   \n",
       "4537  19701d48-d0bd-4a60-8513-07d9449a2b40  MaRisk_v2.pdf   118   \n",
       "4538  6e2b0b8d-599a-4d3f-9077-0c453d99b02a  MaRisk_v2.pdf   119   \n",
       "4539  6294185b-4686-4689-8bd6-12b7bbeedd4a  MaRisk_v2.pdf   119   \n",
       "\n",
       "                                                content  \n",
       "0     I \\n(Legislative acts)  \\nREGULATIONS  \\nREGUL...  \n",
       "1     stronger liquidity buffers. In response to the...  \n",
       "2     national regulatory measures considered to be ...  \n",
       "3     the Council of 14 June 2006 on the capital ade...  \n",
       "4     concerning the access to the activity of insti...  \n",
       "...                                                 ...  \n",
       "4535  the proposals for action with the responsible ...  \n",
       "4536  Bundesanstalt für Finanzdienstleistungsaufsich...  \n",
       "4537  office, and\\ni)in the case of institutions wit...  \n",
       "4538  Bundesanstalt für Finanzdienstleistungsaufsich...  \n",
       "4539  market-oriented institutions shall draw up the...  \n",
       "\n",
       "[4540 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check, if new files has been added to the directory and has to be added to the existing vectorDB.\n",
    "\n",
    "vector_df = store_to_df(db)\n",
    "file_names = os.listdir(DATA_PATH)\n",
    "\n",
    "missing_files = [file for file in file_names if file not in vector_df[\"document\"].values]\n",
    "\n",
    "if not missing_files:\n",
    "    print(\"\\033[92mNo documents are added to the directory. No adjustment of the vector database is required.\\033[0m\")\n",
    "else:\n",
    "    print(\"\\033[92mNew documents has been identified.\\033[0m\")\n",
    "    time.sleep(1)\n",
    "    print(\"\\033[91mThe following documents are new:\\033[0m\", missing_files)\n",
    "    print(\"\\033[91mThe new documents will be added now to the vector database.\\033[0m\")\n",
    "    print(\"\")\n",
    "    time.sleep(1)\n",
    "    print(\"Please wait...\")\n",
    "    print(\"\")\n",
    "   \n",
    "    missing_files_full_path = [os.path.join(DATA_PATH, file) for file in missing_files]\n",
    "    merged_documents = []\n",
    "    for missings in missing_files_full_path:\n",
    "        document_loader = PyPDFLoader(missings)\n",
    "        document_new = document_loader.load()\n",
    "        merged_documents += document_new\n",
    "    print(\"\\033[92mThe new files has been added successfully to the vector database.\\033[0m\")\n",
    "    print(\"\")\n",
    "    time.sleep(1)\n",
    "    print(\"Now the new files will be chunked.\")\n",
    "    text_chunks_new = text_splitter.split_documents(merged_documents)\n",
    "    extension = FAISS.from_documents(text_chunks_new, embeddings)\n",
    "    print(\"\")\n",
    "    print(\"\\033[92mThe text chunking was successful! Now the new chunks will be added to the existing vector database.\\033[0m\")\n",
    "    db.merge_from(extension)\n",
    "    db.save_local('C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/vectorDB') # The created FAISS vector database is stored locally in the given directory.\n",
    "    print(\"\")\n",
    "    print(\"\\033[92mThe merging was successful! The local vector database is now updated. See you next time!\\033[0m\")\n",
    "\n",
    "# Test\n",
    "show_vstore(db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e2f6233a68c3dd44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T09:44:40.177642Z",
     "start_time": "2024-10-29T09:44:37.030968Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[91mDeleted documents in the directory has been identified.\u001b[0m\n",
      "\u001b[91mThe following documents has been deleted:\u001b[0m ['MaRisk_v2.pdf']\n",
      "\n",
      "\u001b[92mThe deleted documents will be deleted now from the vector database.\u001b[0m\n",
      "\n",
      "Please wait...\n",
      "\n",
      "\n",
      "\u001b[92mThe deletion of the relevant documents from the vector database was successful!.\u001b[0m\n",
      "\n",
      "\u001b[92mThe update is completed. See you next time!\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>document</th>\n",
       "      <th>page</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d830feca-520c-496e-ab58-dcd58b70504f</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>I \\n(Legislative acts)  \\nREGULATIONS  \\nREGUL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>af3b5a0c-494f-48ab-9a5c-843854c08145</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>stronger liquidity buffers. In response to the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a688ad37-c9d3-43d0-b3cd-63ba9262a6ad</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>national regulatory measures considered to be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3c44f466-4629-4acf-8952-93aad77b5eb2</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>the Council of 14 June 2006 on the capital ade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4bc26976-f6df-49f3-8384-7a4c98f286b7</td>\n",
       "      <td>CRR.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>concerning the access to the activity of insti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4257</th>\n",
       "      <td>cbc10643-ad89-4170-9b93-c258d8fc5ba1</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>117</td>\n",
       "      <td>the proposals for action with the responsible ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>3cc3400d-138c-4013-8e91-4a3103b2b116</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>118</td>\n",
       "      <td>Bundesanstalt für Finanzdienstleistungsaufsich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>aebe2119-3ea9-4e44-9624-2e8edb1546e3</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>118</td>\n",
       "      <td>office, and\\ni)in the case of institutions wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4260</th>\n",
       "      <td>5ca7e410-6d7f-472c-bc3c-85a3849069f4</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>119</td>\n",
       "      <td>Bundesanstalt für Finanzdienstleistungsaufsich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4261</th>\n",
       "      <td>4a1d6443-e8de-42c4-9af8-eabeba23f29c</td>\n",
       "      <td>MaRisk.pdf</td>\n",
       "      <td>119</td>\n",
       "      <td>market-oriented institutions shall draw up the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4262 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  chunk_id    document  page  \\\n",
       "0     d830feca-520c-496e-ab58-dcd58b70504f     CRR.pdf     1   \n",
       "1     af3b5a0c-494f-48ab-9a5c-843854c08145     CRR.pdf     1   \n",
       "2     a688ad37-c9d3-43d0-b3cd-63ba9262a6ad     CRR.pdf     1   \n",
       "3     3c44f466-4629-4acf-8952-93aad77b5eb2     CRR.pdf     2   \n",
       "4     4bc26976-f6df-49f3-8384-7a4c98f286b7     CRR.pdf     2   \n",
       "...                                    ...         ...   ...   \n",
       "4257  cbc10643-ad89-4170-9b93-c258d8fc5ba1  MaRisk.pdf   117   \n",
       "4258  3cc3400d-138c-4013-8e91-4a3103b2b116  MaRisk.pdf   118   \n",
       "4259  aebe2119-3ea9-4e44-9624-2e8edb1546e3  MaRisk.pdf   118   \n",
       "4260  5ca7e410-6d7f-472c-bc3c-85a3849069f4  MaRisk.pdf   119   \n",
       "4261  4a1d6443-e8de-42c4-9af8-eabeba23f29c  MaRisk.pdf   119   \n",
       "\n",
       "                                                content  \n",
       "0     I \\n(Legislative acts)  \\nREGULATIONS  \\nREGUL...  \n",
       "1     stronger liquidity buffers. In response to the...  \n",
       "2     national regulatory measures considered to be ...  \n",
       "3     the Council of 14 June 2006 on the capital ade...  \n",
       "4     concerning the access to the activity of insti...  \n",
       "...                                                 ...  \n",
       "4257  the proposals for action with the responsible ...  \n",
       "4258  Bundesanstalt für Finanzdienstleistungsaufsich...  \n",
       "4259  office, and\\ni)in the case of institutions wit...  \n",
       "4260  Bundesanstalt für Finanzdienstleistungsaufsich...  \n",
       "4261  market-oriented institutions shall draw up the...  \n",
       "\n",
       "[4262 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check, if files has been deleted from the initial directory and has to be deleted from the existing vectorDB.\n",
    "\n",
    "vector_df = store_to_df(db)\n",
    "file_names = os.listdir(DATA_PATH)\n",
    "\n",
    "deleted_documents = [file for file in vector_df[\"document\"].values if file not in file_names]\n",
    "\n",
    "if not deleted_documents:\n",
    "    print(\"\\033[92mNo Documents has been deleted from the directory. The vector database has not to be updated.\\033[0m\")\n",
    "else:\n",
    "    deleted_documents = list(set(deleted_documents))\n",
    "    print(\"\\033[91mDeleted documents in the directory has been identified.\\033[0m\")\n",
    "    time.sleep(1)\n",
    "    print(\"\\033[91mThe following documents has been deleted:\\033[0m\", deleted_documents)\n",
    "    print(\"\")\n",
    "    time.sleep(1)\n",
    "    print(\"\\033[92mThe deleted documents will be deleted now from the vector database.\\033[0m\")\n",
    "    print(\"\")\n",
    "    time.sleep(1)\n",
    "    print(\"Please wait...\")\n",
    "    print(\"\")\n",
    "    \n",
    "    vector_df = store_to_df(db)\n",
    "    \n",
    "    for deleted_doc in deleted_documents:\n",
    "        chunk_list_delete = vector_df.loc[vector_df[\"document\"] == deleted_doc][\"chunk_id\"].tolist()\n",
    "        db.delete(chunk_list_delete)\n",
    "    print(\"\")\n",
    "    print(\"\\033[92mThe deletion of the relevant documents from the vector database was successful!.\\033[0m\")\n",
    "    db.save_local('C:/Users/49171/OneDrive/Desktop/Programieren/Python/PDF_test/Final/vectorDB')\n",
    "    print(\"\")\n",
    "    print(\"\\033[92mThe update is completed. See you next time!\\033[0m\")\n",
    "# Test\n",
    "show_vstore(db)   \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
